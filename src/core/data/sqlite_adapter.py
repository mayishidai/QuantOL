import aiosqlite
import pandas as pd
import chinese_calendar as calendar
import datetime
from typing import Any, Optional, Dict, List
import os
import chinese_calendar as calendar
import asyncio
import random
import time
from src.support.log.logger import logger
from .database_adapter import DatabaseAdapter


class SQLiteAdapter(DatabaseAdapter):
    """SQLiteæ•°æ®åº“é€‚é…å™¨"""

    def __init__(self, db_path: str = "./data/quantdb.sqlite"):
        self.db_path = db_path
        self.pools: List[aiosqlite.Connection] = []
        self._initialized = False
        self._pool_lock = asyncio.Lock()
        self._pool_index = 0

        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼Œæä¾›é»˜è®¤å€¼
        import os
        self._max_connections = int(os.getenv('SQLITE_MAX_CONNECTIONS', '3'))
        self._busy_timeout = int(os.getenv('SQLITE_BUSY_TIMEOUT', '30000'))
        self._cache_size = int(os.getenv('SQLITE_CACHE_SIZE', '-64000'))
        self._mmap_size = int(os.getenv('SQLITE_MMAP_SIZE', '268435456'))
        self._batch_size = int(os.getenv('SQLITE_BATCH_SIZE', '1000'))
        self._wal_auto_checkpoint = int(os.getenv('SQLITE_WAL_AUTO_CHECKPOINT', '1000'))
        self._journal_limit = int(os.getenv('SQLITE_JOURNAL_LIMIT', '1048576'))

        # æ·»åŠ å®ä¾‹IDç”¨äºè°ƒè¯•
        self._instance_id = id(self)
        self._data_source_manager = None
        logger.info(f"åˆ›å»ºSQLiteAdapterå®ä¾‹ #{self._instance_id} - è¿æ¥æ± :{self._max_connections}, è¶…æ—¶:{self._busy_timeout}ms, æ‰¹é‡å¤§å°:{self._batch_size}")

    def set_data_source_manager(self, data_source_manager):
        """è®¾ç½®æ•°æ®æºç®¡ç†å™¨å¼•ç”¨"""
        self._data_source_manager = data_source_manager
        logger.info(f"SQLiteAdapterå·²è®¾ç½®æ•°æ®æºç®¡ç†å™¨")

    async def initialize(self) -> None:
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥å’Œè¡¨ç»“æ„"""
        logger.info(f"å¼€å§‹åˆå§‹åŒ–SQLiteAdapterå®ä¾‹ #{self._instance_id}")

        if self._initialized:
            logger.info(f"å®ä¾‹ #{self._instance_id} å·²åˆå§‹åŒ–ï¼Œè·³è¿‡")
            return

        async with self._pool_lock:
            if self._initialized:
                logger.info(f"å®ä¾‹ #{self._instance_id} åœ¨é”æ£€æŸ¥æ—¶å·²åˆå§‹åŒ–ï¼Œè·³è¿‡")
                return

            try:
                # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
                data_dir = os.path.dirname(self.db_path)
                if data_dir:
                    os.makedirs(data_dir, exist_ok=True)
                    logger.info(f"ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨: {data_dir}")

                logger.info(f"æ­£åœ¨åˆ›å»ºSQLiteæ•°æ®åº“è¿æ¥æ± : {self.db_path}")

                # åˆ›å»ºè¿æ¥æ± 
                for i in range(self._max_connections):
                    conn = await aiosqlite.connect(self.db_path)
                    await self._configure_connection(conn)
                    self.pools.append(conn)
                    logger.debug(f"åˆ›å»ºè¿æ¥ {i+1}/{self._max_connections}")

                # åˆ›å»ºè¡¨ç»“æ„
                await self._init_db_tables()
                self._initialized = True

                logger.info(f"SQLiteæ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–æˆåŠŸ: {self.db_path} (è¿æ¥æ•°: {len(self.pools)})")

            except Exception as e:
                logger.error(f"SQLiteæ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                raise

    async def _configure_connection(self, conn: aiosqlite.Connection) -> None:
        """é…ç½®æ•°æ®åº“è¿æ¥å‚æ•°"""
        try:
            # å¯ç”¨å¤–é”®çº¦æŸ
            await conn.execute("PRAGMA foreign_keys = ON")

            # åŸºç¡€PRAGMAè®¾ç½®ï¼ˆé¿å…è¿‡åº¦ä¼˜åŒ–å¯¼è‡´çš„é—®é¢˜ï¼‰
            await conn.execute(f"PRAGMA busy_timeout = {self._busy_timeout}")
            await conn.execute("PRAGMA journal_mode = WAL")
            await conn.execute("PRAGMA synchronous = NORMAL")

            logger.debug(f"SQLiteåŸºç¡€è¿æ¥é…ç½®å®Œæˆ - è¶…æ—¶:{self._busy_timeout}ms")

            # å°è¯•è®¾ç½®é«˜çº§é€‰é¡¹ï¼ˆå¦‚æœå¤±è´¥ä¸å½±å“è¿è¡Œï¼‰
            try:
                await conn.execute(f"PRAGMA cache_size = {self._cache_size}")
                await conn.execute("PRAGMA temp_store = MEMORY")
            except Exception as e:
                logger.warning(f"è®¾ç½®åŸºç¡€æ€§èƒ½å‚æ•°å¤±è´¥: {e}")

            try:
                await conn.execute(f"PRAGMA mmap_size = {self._mmap_size}")
                await conn.execute(f"PRAGMA wal_autocheckpoint = {self._wal_auto_checkpoint}")
                await conn.execute(f"PRAGMA journal_size_limit = {self._journal_limit}")
                logger.debug(f"SQLiteé«˜çº§è¿æ¥é…ç½®å®Œæˆ - ç¼“å­˜:{self._cache_size}KB, MMAP:{self._mmap_size}å­—èŠ‚")
            except Exception as e:
                logger.warning(f"è®¾ç½®é«˜çº§æ€§èƒ½å‚æ•°å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"SQLiteè¿æ¥é…ç½®å¤±è´¥: {e}")
            # é…ç½®å¤±è´¥ä¸åº”è¯¥é˜»æ­¢ç³»ç»Ÿè¿è¡Œ
            pass

    async def create_connection_pool(self) -> aiosqlite.Connection:
        """åˆ›å»ºè¿æ¥æ± ï¼ˆä¸ºäº†å…¼å®¹æŠ½è±¡ç±»æ¥å£ï¼‰"""
        # ä¸ºäº†ä¿æŒå‘åå…¼å®¹ï¼Œè¿”å›ç¬¬ä¸€ä¸ªè¿æ¥
        if not self._initialized:
            await self.initialize()

        if not self.pools:
            raise RuntimeError("è¿æ¥æ± æœªåˆå§‹åŒ–")

        return self.pools[0]

    async def _get_connection(self) -> aiosqlite.Connection:
        """ä»è¿æ¥æ± è·å–è¿æ¥"""
        if not self._initialized:
            # ä¸åœ¨è¿™é‡Œè°ƒç”¨initialize()ï¼Œé¿å…é€’å½’
            raise RuntimeError("æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨initialize()")

        async with self._pool_lock:
            if not self.pools:
                raise RuntimeError("è¿æ¥æ± æœªåˆå§‹åŒ–")

            # è½®è¯¢è·å–è¿æ¥
            conn = self.pools[self._pool_index]
            self._pool_index = (self._pool_index + 1) % len(self.pools)
            return conn

    async def _execute_with_retry(self, conn: aiosqlite.Connection, query: str, parameters=None, max_retries=3):
        """å¸¦é‡è¯•æœºåˆ¶çš„æ•°æ®åº“æ“ä½œæ‰§è¡Œ"""
        for attempt in range(max_retries):
            try:
                if parameters is None:
                    return await conn.execute(query)
                else:
                    return await conn.execute(query, parameters)
            except Exception as e:
                error_msg = str(e).lower()

                # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®åº“é”å®šé”™è¯¯
                if any(keyword in error_msg for keyword in ['database is locked', 'database locked', 'sqlite_busy']):
                    if attempt == max_retries - 1:
                        logger.error(f"æ•°æ®åº“æ“ä½œæœ€ç»ˆå¤±è´¥ï¼Œé‡è¯•{max_retries}æ¬¡åä»é”å®š: {query[:100]}")
                        raise

                    # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
                    delay = (2 ** attempt) + random.uniform(0, 1)
                    logger.warning(f"æ•°æ®åº“é”å®šï¼Œç¬¬{attempt+1}æ¬¡é‡è¯•ï¼Œç­‰å¾…{delay:.2f}ç§’: {query[:100]}")
                    await asyncio.sleep(delay)
                else:
                    # éé”å®šé”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    raise

    async def _executemany_with_retry(self, conn: aiosqlite.Connection, query: str, parameters=None, max_retries=8):
        """å¸¦é‡è¯•æœºåˆ¶çš„æ‰¹é‡æ•°æ®åº“æ“ä½œæ‰§è¡Œï¼ˆå¢å¼ºç‰ˆ - è§£å†³é”å®šé—®é¢˜ï¼‰"""
        for attempt in range(max_retries):
            try:
                if parameters is None:
                    return await conn.executemany(query)
                else:
                    return await conn.executemany(query, parameters)
            except Exception as e:
                error_msg = str(e).lower()

                # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®åº“é”å®šé”™è¯¯
                if any(keyword in error_msg for keyword in ['database is locked', 'database locked', 'sqlite_busy']):
                    if attempt == max_retries - 1:
                        logger.error(f"æ‰¹é‡æ•°æ®åº“æ“ä½œæœ€ç»ˆå¤±è´¥ï¼Œé‡è¯•{max_retries}æ¬¡åä»é”å®š: {query[:100]}")
                        raise

                    # æ”¹è¿›çš„é€€é¿ç­–ç•¥ï¼šæ›´é•¿çš„ç­‰å¾…æ—¶é—´ï¼Œç‰¹åˆ«æ˜¯å‰å‡ æ¬¡é‡è¯•
                    if attempt < 2:
                        # å‰ä¸¤æ¬¡é‡è¯•ä½¿ç”¨è¾ƒçŸ­å»¶è¿Ÿ
                        delay = 0.5 + random.uniform(0, 0.5)
                    elif attempt < 4:
                        # ä¸­é—´é‡è¯•ä½¿ç”¨ä¸­ç­‰å»¶è¿Ÿ
                        delay = 2 + random.uniform(0, 1)
                    else:
                        # åç»­é‡è¯•ä½¿ç”¨æ›´é•¿å»¶è¿Ÿ
                        delay = 5 + random.uniform(0, 2)

                    logger.warning(f"æ‰¹é‡æ•°æ®åº“æ“ä½œé”å®šï¼Œç¬¬{attempt+1}æ¬¡é‡è¯•ï¼Œç­‰å¾…{delay:.2f}ç§’: {query[:100]}")
                    await asyncio.sleep(delay)
                else:
                    # éé”å®šé”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    raise

    async def execute_query(self, query: str, *args) -> Any:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        conn = await self._get_connection()

        try:
            # è½¬æ¢PostgreSQLè¯­æ³•åˆ°SQLiteè¯­æ³•
            sqlite_query = self._convert_query_syntax(query)

            if query.strip().upper().startswith('SELECT'):
                cursor = await self._execute_with_retry(conn, sqlite_query, args if args else ())
                rows = await cursor.fetchall()
                # è·å–åˆ—å
                columns = [description[0] for description in cursor.description] if cursor.description else []
                return [dict(zip(columns, row)) for row in rows]
            else:
                await self._execute_with_retry(conn, sqlite_query, args if args else ())
                # æäº¤äº‹åŠ¡ï¼Œé‡Šæ”¾é”
                await conn.commit()
                return None

        except Exception as e:
            # å›æ»šäº‹åŠ¡ï¼Œé‡Šæ”¾é”
            try:
                await conn.rollback()
            except:
                pass
            logger.error(f"SQLiteæŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise

    def _convert_query_syntax(self, query: str) -> str:
        """è½¬æ¢PostgreSQLè¯­æ³•åˆ°SQLiteè¯­æ³•"""
        # å¤„ç†æ•°æ®ç±»å‹å·®å¼‚
        query = query.replace('SERIAL', 'INTEGER PRIMARY KEY AUTOINCREMENT')
        query = query.replace('NUMERIC', 'REAL')
        query = query.replace('VARCHAR', 'TEXT')

        # å¤„ç†å‡½æ•°å·®å¼‚
        query = query.replace('NOW()', "datetime('now')")
        query = query.replace('TRUE', '1')
        query = query.replace('FALSE', '0')

        # å¤„ç†ON CONFLICTè¯­æ³•
        if 'ON CONFLICT' in query:
            query = self._convert_on_conflict_syntax(query)

        # å¤„ç†RETURNINGå­å¥ï¼ˆSQLiteä¸æ”¯æŒï¼‰
        if 'RETURNING' in query:
            query = query.split('RETURNING')[0]

        return query

    def _convert_on_conflict_syntax(self, query: str) -> str:
        """è½¬æ¢ON CONFLICTè¯­æ³•"""
        # ç®€åŒ–çš„ON CONFLICTè½¬æ¢ï¼Œå®é™…æƒ…å†µå¯èƒ½éœ€è¦æ›´å¤æ‚çš„å¤„ç†
        if 'ON CONFLICT (code, date, time, frequency) DO UPDATE SET' in query:
            query = query.replace('ON CONFLICT (code, date, time, frequency) DO UPDATE SET',
                                'ON CONFLICT(code, date, time, frequency) DO UPDATE SET')
            query = query.replace('EXCLUDED.', 'new.')
        elif 'ON CONFLICT (code) DO UPDATE SET' in query:
            query = query.replace('ON CONFLICT (code) DO UPDATE SET',
                                'ON CONFLICT(code) DO UPDATE SET')
            query = query.replace('EXCLUDED.', 'new.')
        elif 'ON CONFLICT (stat_month) DO UPDATE SET' in query:
            query = query.replace('ON CONFLICT (stat_month) DO UPDATE SET',
                                'ON CONFLICT(stat_month) DO UPDATE SET')
            query = query.replace('EXCLUDED.', 'new.')

        return query

    async def close(self) -> None:
        """å…³é—­è¿æ¥æ± """
        async with self._pool_lock:
            if self.pools:
                logger.info(f"å…³é—­{len(self.pools)}ä¸ªSQLiteè¿æ¥")
                for conn in self.pools:
                    await conn.close()
                self.pools.clear()
                self._initialized = False

    async def _init_db_tables(self):
        """åˆå§‹åŒ–è¡¨ç»“æ„ - æç®€ç‰ˆæœ¬"""
        logger.info("å¼€å§‹SQLiteè¡¨ç»“æ„åˆå§‹åŒ–...")

        try:
            # ç›´æ¥ä½¿ç”¨ç¬¬ä¸€ä¸ªè¿æ¥ï¼Œé¿å…è·å–è¿æ¥çš„æ­»é”é—®é¢˜
            logger.info("ğŸ”§ ä½¿ç”¨ç¬¬ä¸€ä¸ªè¿æ¥è¿›è¡Œåˆå§‹åŒ–...")
            if not self.pools:
                raise RuntimeError("è¿æ¥æ± ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆå§‹åŒ–")

            conn = self.pools[0]  # ç›´æ¥ä½¿ç”¨ç¬¬ä¸€ä¸ªè¿æ¥
            logger.info("âœ… è·å–åˆ°è¿æ¥ç”¨äºè¡¨åˆå§‹åŒ–")

            # æ ¹æ®å­—æ®µæ˜ å°„æ–‡æ¡£åˆ›å»ºæ ‡å‡†è¡¨ç»“æ„
            logger.info("ğŸ”¨ å¼€å§‹åˆ›å»ºStockInfoè¡¨...")

            # åˆ›å»ºç¬¦åˆå¤šæ•°æ®æºçš„StockInfoè¡¨
            sql = """
                CREATE TABLE IF NOT EXISTS StockInfo (
                    code TEXT PRIMARY KEY,           -- ç»Ÿä¸€è‚¡ç¥¨ä»£ç ï¼ˆä¸å¸¦äº¤æ˜“æ‰€åç¼€ï¼‰
                    code_name TEXT NOT NULL,        -- è‚¡ç¥¨åç§°
                    ipoDate TEXT NOT NULL,         -- ä¸Šå¸‚æ—¥æœŸ (YYYY-MM-DD)
                    outDate TEXT,                   -- é€€å¸‚æ—¥æœŸ (YYYY-MM-DD)ï¼Œnullè¡¨ç¤ºæœªé€€å¸‚
                    type TEXT DEFAULT 'è‚¡ç¥¨',      -- è‚¡ç¥¨ç±»å‹
                    status TEXT DEFAULT 'ä¸Šå¸‚',    -- ä¸Šå¸‚çŠ¶æ€ï¼šä¸Šå¸‚/é€€å¸‚/æš‚åœ
                    market TEXT,                    -- äº¤æ˜“æ‰€ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
                    data_source TEXT DEFAULT '',   -- æ•°æ®æ¥æºæ ‡è¯†
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    updated_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """

            await conn.execute(sql)
            logger.info("âœ… StockInfoè¡¨åˆ›å»ºæˆåŠŸ")

            # åˆ›å»ºStockDataè¡¨ï¼ˆä¿æŒç®€å•ç»“æ„ï¼‰
            logger.info("ğŸ”¨ å¼€å§‹åˆ›å»ºStockDataè¡¨...")
            sql2 = """
                CREATE TABLE IF NOT EXISTS StockData (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    code TEXT NOT NULL,              -- è‚¡ç¥¨ä»£ç 
                    date TEXT NOT NULL,              -- æ—¥æœŸ
                    time TEXT NOT NULL,              -- æ—¶é—´
                    open REAL NOT NULL,              -- å¼€ç›˜ä»·
                    high REAL NOT NULL,              -- æœ€é«˜ä»·
                    low REAL NOT NULL,               -- æœ€ä½ä»·
                    close REAL NOT NULL,             -- æ”¶ç›˜ä»·
                    volume INTEGER NOT NULL,         -- æˆäº¤é‡
                    amount REAL,                     -- æˆäº¤é¢
                    adjustflag TEXT,                 -- å¤æƒçŠ¶æ€
                    frequency TEXT NOT NULL,         -- æ•°æ®é¢‘ç‡
                    data_source TEXT DEFAULT '',     -- æ•°æ®æ¥æºæ ‡è¯†
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE (code, date, time, frequency)
                )
            """

            await conn.execute(sql2)
            logger.info("âœ… StockDataè¡¨åˆ›å»ºæˆåŠŸ")

            logger.info("ğŸ‰ SQLiteè¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ SQLiteè¡¨ç»“æ„åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            raise

    async def save_stock_info(self, code: str, code_name: str, ipo_date: str,
                             stock_type: str, status: str, out_date: Optional[str] = None) -> bool:
        """ä¿å­˜è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯åˆ°StockInfoè¡¨"""
        conn = await self._get_connection()
        try:
            await self._execute_with_retry(conn, """
                INSERT OR REPLACE INTO StockInfo (code, code_name, ipoDate, outDate, type, status)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (code, code_name, ipo_date, out_date, stock_type, status))

            logger.info(f"æˆåŠŸä¿å­˜è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯: {code}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {str(e)}")
            raise

    async def check_data_completeness(self, symbol: str, start_date: datetime.date, end_date: datetime.date, frequency: str) -> list:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        try:
            # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
            if isinstance(start_date, str):
                start_dt = pd.to_datetime(start_date).date()
            else:
                start_dt = start_date

            if isinstance(end_date, str):
                end_dt = pd.to_datetime(end_date).date()
            else:
                end_dt = end_date

            logger.info(f"Checking data completeness for {symbol} from {start_dt} to {end_dt}")

            conn = await self._get_connection()
            # è·å–æ•°æ®åº“ä¸­å·²æœ‰æ—¥æœŸ
            query = """
                SELECT DISTINCT date
                FROM StockData
                WHERE code = ? AND frequency = ? AND date BETWEEN ? AND ?
                ORDER BY date
            """
            cursor = await self._execute_with_retry(conn, query, (symbol, frequency, start_dt, end_dt))
            rows = await cursor.fetchall()
            logger.info(f"ä»æ•°æ®åº“è·å– {start_dt}-{end_dt} for {symbol}")

            existing_dates = {pd.to_datetime(row[0]).date() for row in rows}

            # ç”Ÿæˆç†è®ºäº¤æ˜“æ—¥é›†åˆï¼ˆæ’é™¤èŠ‚å‡æ—¥ï¼‰
            all_dates = pd.date_range(start_dt, end_dt, freq='B')  # å·¥ä½œæ—¥
            trading_dates = set(
                date.date() for date in all_dates
                if not calendar.is_holiday(date.date())
            )
            today = datetime.date.today()
            trading_dates = {d for d in trading_dates if d != today}  # è‹¥ä»Šæ—¥æŸ¥è¯¢ï¼Œåˆ™æ’é™¤ä»Šæ—¥

            # è®¡ç®—ç¼ºå¤±æ—¥æœŸ
            missing_dates = trading_dates - existing_dates

            # å°†è¿ç»­ç¼ºå¤±æ—¥æœŸåˆå¹¶ä¸ºåŒºé—´
            missing_ranges = []
            if missing_dates:
                sorted_dates = sorted(missing_dates)
                range_start = sorted_dates[0]
                prev_date = range_start

                for current_date in sorted_dates[1:]:
                    if (current_date - prev_date).days > 1:  # å‡ºç°æ–­ç‚¹
                        missing_ranges.append((range_start, prev_date))
                        range_start = current_date
                    prev_date = current_date

                # æ·»åŠ æœ€åä¸€ä¸ªåŒºé—´
                missing_ranges.append((range_start, prev_date))

            logger.info(f"Found {len(missing_ranges)} missing data ranges for {symbol}")
            return missing_ranges

        except Exception as e:
            logger.error(f"æ£€æŸ¥æ•°æ®å®Œæ•´æ€§å¤±è´¥: {str(e)}")
            raise

    async def load_stock_data(self, symbol: str, start_date: datetime.date, end_date: datetime.date, frequency: str) -> pd.DataFrame:
        """ä»æ•°æ®åº“åŠ è½½è‚¡ç¥¨æ•°æ®"""
        try:
            # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
            if isinstance(start_date, str):
                start_dt = pd.to_datetime(start_date).date()
            else:
                start_dt = start_date

            if isinstance(end_date, str):
                end_dt = pd.to_datetime(end_date).date()
            else:
                end_dt = end_date

            logger.info(f"Loading stock data for {symbol} from {start_dt} to {end_dt}")

            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            missing_ranges = await self.check_data_completeness(symbol, start_dt, end_dt, frequency)
            logger.info(f"æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å®Œæˆï¼Œå‘ç° {len(missing_ranges)} ä¸ªç¼ºå¤±åŒºé—´")

            # å¦‚æœæœ‰ç¼ºå¤±æ•°æ®ï¼Œä»é€‰æ‹©çš„æ•°æ®æºè·å–å¹¶ä¿å­˜
            if missing_ranges:
                logger.info(f"Fetching missing data ranges for {symbol}")

                # è·å–å½“å‰é€‰æ‹©çš„æ•°æ®æº
                try:
                    from .config.data_source_config import get_data_source_manager
                    data_source_manager = get_data_source_manager()

                    # å…ˆå°è¯•ä»ç¯å¢ƒå˜é‡æ›´æ–°Tushareé…ç½®
                    data_source_manager.update_tushare_token_from_env()

                    current_source = data_source_manager.get_current_data_source()
                    logger.info(f"ä½¿ç”¨å½“å‰é€‰æ‹©çš„æ•°æ®æº: {current_source}")

                    if current_source and current_source.lower() == 'tushare':
                        logger.info("å°è¯•ä½¿ç”¨Tushareæ•°æ®æº")
                        # ä½¿ç”¨Tushareæ•°æ®æº
                        from .adapters.tushare_service_adapter import TushareServiceAdapter
                        from .config.tushare_config import TushareConfig

                        # é‡æ–°è·å–æ›´æ–°åçš„Tushareé…ç½®
                        tushare_config = data_source_manager.get_data_source('Tushare')
                        logger.debug(f"è·å–Tushareé…ç½®: {tushare_config}")

                        if tushare_config and tushare_config.settings.enabled and tushare_config.credentials.token:
                            logger.info(f"Tushareé…ç½®å®Œæ•´ï¼Œtoken: {tushare_config.credentials.token[:8]}...ï¼Œåˆ›å»ºTushareServiceAdapter")
                            config = TushareConfig(
                                token=tushare_config.credentials.token,
                                cache_enabled=tushare_config.settings.cache_enabled,
                                cache_ttl=tushare_config.settings.cache_ttl,
                                rate_limit=tushare_config.settings.rate_limit
                            )
                            data_source = TushareServiceAdapter(config)
                            logger.info("TushareServiceAdapteråˆ›å»ºæˆåŠŸ")
                        else:
                            logger.warning(f"Tushareæ•°æ®æºæœªå¯ç”¨æˆ–é…ç½®ä¸å®Œæ•´ï¼Œå›é€€åˆ°Baostockã€‚enabled: {tushare_config.settings.enabled if tushare_config else 'None'}, token: {'æœ‰' if tushare_config and tushare_config.credentials.token else 'æ— '}")
                            from .baostock_source import BaostockDataSource
                            data_source = BaostockDataSource(frequency)
                    else:
                        logger.warning(f"å½“å‰æ•°æ®æºä¸æ˜¯Tushare ({current_source})ï¼Œä½¿ç”¨é»˜è®¤Baostockæ•°æ®æº")
                        # é»˜è®¤ä½¿ç”¨Baostockæ•°æ®æº
                        from .baostock_source import BaostockDataSource
                        data_source = BaostockDataSource(frequency)

                except Exception as e:
                    logger.error(f"è·å–æ•°æ®æºé…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤Baostock: {str(e)}")
                    import traceback
                    logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                    from .baostock_source import BaostockDataSource
                    data_source = BaostockDataSource(frequency)

                data = pd.DataFrame()
                for range_start, range_end in missing_ranges:
                    logger.info(f"Fetching data from {range_start} to {range_end} using {current_source}")
                    new_data = await data_source.load_data(symbol, range_start, range_end, frequency)
                    await self.save_stock_data(symbol, new_data, frequency)
                    data = pd.concat([data, new_data])
            else:
                logger.info(f"æ•°æ®å®Œæ•´ï¼Œæ— éœ€ä»å¤–éƒ¨æ•°æ®æºè·å– {symbol} çš„æ•°æ®")

            # ä»æ•°æ®åº“åŠ è½½å®Œæ•´æ•°æ®
            query = """
                SELECT date, time, code, open, high, low, close, volume, amount, adjustflag, frequency
                FROM StockData
                WHERE code = ?
                AND date BETWEEN ? AND ?
                AND frequency = ?
                ORDER BY date
            """

            async with aiosqlite.connect(self.db_path) as conn:
                cursor = await self._execute_with_retry(conn, query, (symbol, start_dt, end_dt, frequency))
                rows = await cursor.fetchall()
            logger.info(f"æ•°æ®åº“æŸ¥è¯¢å®Œæˆï¼Œè¿”å› {len(rows) if rows else 0} è¡Œæ•°æ®")

            if not rows:
                logger.warning(f"[{symbol}] æœªæ‰¾åˆ°è‚¡ç¥¨æ•°æ® date_range=[{start_date}~{end_date}] frequency={frequency}")
                return pd.DataFrame()

            data = [row for row in rows]
            df = pd.DataFrame(data, columns=['date', 'time', 'code', 'open', 'high', 'low', 'close',
                                            'volume', 'amount', 'adjustflag', 'frequency'])
            df = self._transform_data(df)

            logger.info(f"Successfully loaded {len(df)} rows for {symbol}")
            return df

        except Exception as e:
            logger.error(f"Failed to load stock data: {str(e)}")
            raise

    def _transform_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–æ•°æ®æ ¼å¼"""
        # ä¸PostgreSQLç‰ˆæœ¬ç›¸åŒçš„è½¬æ¢é€»è¾‘
        if 'date' in data.columns:
            data['date'] = pd.to_datetime(data['date'])
            data['date'] = data['date'].dt.strftime('%Y-%m-%d')

        if 'time' in data.columns:
            if data['time'].isna().any():
                logger.warning(f"å‘ç° {data['time'].isna().sum()} ä¸ªNaTå€¼åœ¨timeåˆ—")
                data.loc[data['time'].isna(), 'time'] = '00:00:00'

            data['time'] = data['time'].astype(str)
            data['time'] = data['time'].apply(lambda x: x if len(x) >= 8 else '00:00:00')

        if 'frequency' in data.columns:
            if data['frequency'].isna().any():
                logger.warning(f"å‘ç° {data['frequency'].isna().sum()} ä¸ªNaNå€¼åœ¨frequencyåˆ—")
                data.loc[data['frequency'].isna(), 'frequency'] = 'd'

        if 'date' in data.columns and 'time' in data.columns:
            try:
                data['date'] = data['date'].astype(str)
                data['time'] = data['time'].astype(str)
                data['combined_time'] = data['date'] + ' ' + data['time']
                data['combined_time'] = pd.to_datetime(
                    data['combined_time'],
                    format='%Y-%m-%d %H:%M:%S',
                    errors='coerce'
                )

                if data['combined_time'].isna().any():
                    failed_count = data['combined_time'].isna().sum()
                    logger.warning(f"å‘ç° {failed_count} ä¸ªcombined_timeè½¬æ¢å¤±è´¥")
                    mask = data['combined_time'].isna()
                    data.loc[mask, 'combined_time'] = pd.to_datetime(
                        data.loc[mask, 'date'] + ' 00:00:00',
                        format='%Y-%m-%d %H:%M:%S'
                    )

            except Exception as e:
                logger.error(f"åˆ›å»ºcombined_timeåˆ—å¤±è´¥: {str(e)}")
                data['combined_time'] = pd.to_datetime(data['date'])

        if 'combined_time' in data.columns:
            data = data.sort_values(by='combined_time').reset_index(drop=True)

        return data

    async def get_all_stocks(self) -> pd.DataFrame:
        """è·å–æ‰€æœ‰è‚¡ç¥¨ä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            logger.debug("æ£€æŸ¥æ•°æ®æ˜¯å¦æœ€æ–°")
            if await self._is_stock_info_up_to_date():
                # ä»æ•°æ®åº“è¯»å–ç°æœ‰æ•°æ®
                conn = await self._get_connection()
                cursor = await self._execute_with_retry(conn, "SELECT * FROM StockInfo")
                rows = await cursor.fetchall()

                # åŠ¨æ€è·å–åˆ—å
                columns = [description[0] for description in cursor.description] if cursor.description else []
                logger.debug(f"ä»æ•°æ®åº“è·å–åˆ°{len(columns)}åˆ—: {columns}")

                return pd.DataFrame(rows, columns=columns)
            else:
                # éœ€è¦æ›´æ–°æ•°æ®
                logger.info("StockInfoè¡¨æ•°æ®è¿‡æœŸï¼Œå¼€å§‹æ›´æ–°")

                # ä½¿ç”¨æ•°æ®æºç®¡ç†å™¨è·å–å½“å‰é€‰æ‹©çš„æ•°æ®æº
                selected_data_source = None
                if self._data_source_manager:
                    selected_data_source = self._data_source_manager.get_current_data_source()
                    logger.info(f"ä»æ•°æ®æºç®¡ç†å™¨è·å–å½“å‰æ•°æ®æº: {selected_data_source}")
                else:
                    # é™çº§åˆ°ç¯å¢ƒå˜é‡è¯»å–
                    selected_data_source = os.getenv('SELECTED_DATA_SOURCE', 'baostock')
                    logger.info(f"ä»ç¯å¢ƒå˜é‡è·å–æ•°æ®æºï¼ˆé™çº§æ¨¡å¼ï¼‰: {selected_data_source}")

                # æ ‡å‡†åŒ–æ•°æ®æºåç§°ï¼ˆåŒ¹é…ç³»ç»Ÿè®¾ç½®ä¸­çš„é€‰é¡¹ï¼‰
                if selected_data_source == 'Tushare':
                    data_source_type = 'tushare'
                elif selected_data_source == 'Baostock':
                    data_source_type = 'baostock'
                else:
                    data_source_type = 'baostock'  # é»˜è®¤

                if data_source_type == 'tushare':
                    # ä½¿ç”¨Tushareæ•°æ®æº
                    from .adapters.tushare_adapter import TushareAdapter
                    from .config.tushare_config import TushareConfig

                    try:
                        token = os.getenv('TUSHARE_TOKEN')
                        if not token:
                            raise ValueError("Tushare tokenæœªé…ç½®")

                        config = TushareConfig(token=token)
                        adapter = TushareAdapter(config.token)
                        df = adapter.get_stock_basic()

                        # æ ‡è®°æ•°æ®æºç±»å‹
                        df['data_source'] = 'tushare'
                        logger.info(f"ä»Tushareè·å–åˆ° {len(df)} æ¡è‚¡ç¥¨æ•°æ®")

                    except Exception as e:
                        logger.error(f"Tushareæ•°æ®æºè·å–å¤±è´¥ï¼Œåˆ‡æ¢åˆ°Baostock: {e}")
                        # é™çº§åˆ°Baostock
                        from .baostock_source import BaostockDataSource
                        data_source = BaostockDataSource()
                        df = await data_source._get_all_stocks()
                        df['data_source'] = 'baostock'

                else:
                    # é»˜è®¤ä½¿ç”¨Baostockæ•°æ®æº
                    from .baostock_source import BaostockDataSource
                    data_source = BaostockDataSource()
                    df = await data_source._get_all_stocks()
                    df['data_source'] = 'baostock'

                # å°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“
                await self._update_stock_info(df)
                return df
        except Exception as e:
            logger.error(f"è·å–æ‰€æœ‰è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {str(e)}")
            raise

    async def _is_stock_info_up_to_date(self, max_retries: int = 3) -> bool:
        """æ£€æŸ¥StockInfoè¡¨æ˜¯å¦æœ€æ–°"""
        conn = await self._get_connection()

        for attempt in range(max_retries):
            try:
                logger.debug(f"æ£€æŸ¥StockInfoè¡¨çŠ¶æ€(å°è¯•{attempt+1}/{max_retries})")

                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                cursor = await self._execute_with_retry(conn, """
                    SELECT name FROM sqlite_master
                    WHERE type='table' AND name='StockInfo'
                """)
                table_exists = await cursor.fetchone()

                if not table_exists:
                    logger.warning("è¡¨StockInfoä¸å­˜åœ¨")
                    return False

                # æ£€æŸ¥æœ€æ–°IPOæ—¥æœŸ
                cursor = await self._execute_with_retry(conn, """
                    SELECT ipoDate FROM StockInfo
                    ORDER BY ipoDate DESC LIMIT 1
                """)
                row = await cursor.fetchone()

                if not row:
                    logger.warning("StockInfoè¡¨ä¸ºç©º")
                    return False

                latest_ipo = pd.Timestamp(row[0])
                cutoff = pd.Timestamp.now() - pd.Timedelta(days=30)
                is_up_to_date = latest_ipo >= cutoff

                logger.debug(f"æœ€æ–°IPOæ—¥æœŸ: {latest_ipo.isoformat()}, æ˜¯å¦æœ€æ–°: {is_up_to_date}")
                return is_up_to_date

            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"æ£€æŸ¥StockInfoè¡¨çŠ¶æ€å¤±è´¥(æœ€ç»ˆå°è¯•): {str(e)}")
                    raise
                logger.warning(f"æ£€æŸ¥StockInfoè¡¨çŠ¶æ€å¤±è´¥(å°è¯•{attempt+1}): {str(e)}")
                await asyncio.sleep(1 * (attempt + 1))
        return False

    async def _update_stock_info(self, df: pd.DataFrame) -> tuple:
        """æ›´æ–°StockInfoè¡¨æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆ - è§£å†³é”å®šé—®é¢˜ï¼‰"""
        valid_data = []
        invalid_rows = []

        try:
            # éªŒè¯æ‰€æœ‰æ•°æ®è¡Œ
            for _, row in df.iterrows():
                try:
                    validated_row = await self._validate_stock_info(row)
                    valid_data.append(validated_row)
                except Exception as e:
                    invalid_rows.append((row.to_dict(), str(e)))

            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œæå‰è¿”å›
            if not valid_data:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯æ’å…¥StockInfoè¡¨")
                return 0, len(invalid_rows)

            # ä½¿ç”¨ç‹¬ç«‹è¿æ¥è¿›è¡Œäº‹åŠ¡å¤„ç†
            conn = await self._get_connection()

            # æ‰‹åŠ¨ç®¡ç†äº‹åŠ¡
            try:
                # å¼€å§‹äº‹åŠ¡
                await conn.execute("BEGIN")
                logger.debug("å¼€å§‹äº‹åŠ¡æ›´æ–°StockInfoè¡¨")

                # æ¸…ç©ºç°æœ‰æ•°æ®
                logger.debug("æ¸…ç©ºç°æœ‰StockInfoè¡¨æ•°æ®")
                await self._execute_with_retry(conn, "DELETE FROM StockInfo")

                # åˆ†æ‰¹æ’å…¥æ•°æ®ä»¥å‡å°‘é”æŒæœ‰æ—¶é—´
                batch_size = self._batch_size  # ä½¿ç”¨é…ç½®çš„æ‰¹é‡å¤§å°
                total_inserted = 0

                for i in range(0, len(valid_data), batch_size):
                    batch = valid_data[i:i + batch_size]
                    logger.debug(f"æ’å…¥ç¬¬{i//batch_size + 1}æ‰¹æ•°æ®ï¼Œå…±{len(batch)}æ¡è®°å½•")

                    try:
                        await self._executemany_with_retry(conn, """
                            INSERT INTO StockInfo (code, code_name, ipoDate, outDate, type, status, data_source)
                            VALUES (?, ?, ?, ?, ?, ?, ?)
                        """, parameters=batch)
                        total_inserted += len(batch)

                        # åœ¨æ‰¹æ¬¡ä¹‹é—´çŸ­æš‚æš‚åœï¼Œè®©å…¶ä»–æ“ä½œæœ‰æœºä¼šè®¿é—®æ•°æ®åº“
                        if i + batch_size < len(valid_data):
                            await asyncio.sleep(0.01)  # 10msæš‚åœ

                    except Exception as e:
                        logger.error(f"æ‰¹æ¬¡æ’å…¥å¤±è´¥: {str(e)}")
                        # å›æ»šäº‹åŠ¡
                        await conn.execute("ROLLBACK")
                        raise

                # æäº¤äº‹åŠ¡
                await conn.execute("COMMIT")
                logger.info(f"æˆåŠŸæ›´æ–°StockInfoè¡¨æ•°æ®ï¼ŒæˆåŠŸæ’å…¥{total_inserted}è¡Œï¼Œå¤±è´¥{len(invalid_rows)}è¡Œ")
                return total_inserted, len(invalid_rows)

            except Exception as e:
                # ç¡®ä¿äº‹åŠ¡è¢«å›æ»š
                try:
                    await conn.execute("ROLLBACK")
                except:
                    pass  # å›æ»šå¤±è´¥ï¼Œè¿æ¥å¯èƒ½å·²ç»å…³é—­
                raise

        except Exception as e:
            logger.error(f"æ›´æ–°StockInfoè¡¨å¤±è´¥: {str(e)}")
            raise

    async def _validate_stock_info(self, row: pd.Series) -> tuple:
        """éªŒè¯å¹¶è½¬æ¢è‚¡ç¥¨ä¿¡æ¯æ ¼å¼ï¼ˆæ”¯æŒå¤šæ•°æ®æºå­—æ®µï¼‰"""
        try:
            # ä½¿ç”¨å­—æ®µæ˜ å°„å™¨è¿›è¡ŒéªŒè¯
            from .field_mapper import FieldMapper

            # å°†rowè½¬æ¢ä¸ºDataFrameè¿›è¡ŒéªŒè¯
            df = pd.DataFrame([row])

            # æ˜ å°„å­—æ®µå¹¶éªŒè¯
            mapped_df = FieldMapper.map_to_standard_fields(df, row.get('data_source', 'baostock'))
            is_valid, issues = FieldMapper.validate_required_fields(mapped_df)

            if not is_valid:
                raise ValueError(f"å­—æ®µéªŒè¯å¤±è´¥: {issues}")

            # è·å–éªŒè¯åçš„æ•°æ®
            mapped_row = mapped_df.iloc[0]

            # éªŒè¯ipoDateæ ¼å¼
            if not isinstance(mapped_row['ipo_date'], str):
                raise ValueError(f"Invalid ipoDate format: {mapped_row['ipo_date']}")

            try:
                ipo_date = pd.to_datetime(mapped_row['ipo_date'], format='%Y-%m-%d', errors='coerce')
                if pd.isna(ipo_date):
                    raise ValueError(f"Invalid ipoDate value: {mapped_row['ipo_date']}")
                ipo_date_str = ipo_date.strftime('%Y-%m-%d')
            except Exception as e:
                raise ValueError(f"ipoDateè½¬æ¢å¤±è´¥: {e}")

            # å¤„ç†outDate
            out_date_str = None
            if pd.notna(mapped_row.get('out_date')) and mapped_row['out_date'] != '':
                try:
                    out_date = pd.to_datetime(mapped_row['out_date'], format='%Y-%m-%d', errors='coerce')
                    if pd.notna(out_date):
                        out_date_str = out_date.strftime('%Y-%m-%d')
                except Exception as e:
                    logger.warning(f"outDateè½¬æ¢å¤±è´¥ï¼Œè®¾ä¸ºnull: {e}")
                    out_date_str = None

            # æ„å»ºæ’å…¥å…ƒç»„
            return (
                str(mapped_row['code']),
                str(mapped_row['code_name']),
                ipo_date_str,
                out_date_str,
                str(mapped_row['type']),
                str(mapped_row['status']),
                str(mapped_row.get('data_source', ''))
            )
        except Exception as e:
            logger.error(f"æ•°æ®éªŒè¯å¤±è´¥: {str(e)} - è¡Œæ•°æ®: {row.to_dict()}")
            raise

    async def get_stock_info(self, code: str) -> dict:
        """è·å–è‚¡ç¥¨å®Œæ•´ä¿¡æ¯"""
        conn = await self._get_connection()
        try:
            cursor = await self._execute_with_retry(conn, """
                SELECT code_name, ipoDate, outDate, type, status
                FROM StockInfo
                WHERE code = ?
            """, (code,))

            row = await cursor.fetchone()
            if not row:
                return {}

            return {
                "code_name": row[0],
                "ipo_date": row[1],
                "out_date": row[2],
                "type": row[3],
                "status": row[4]
            }
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {str(e)}")
            raise

    async def get_stock_name(self, code: str) -> str:
        """æ ¹æ®è‚¡ç¥¨ä»£ç è·å–åç§°"""
        conn = await self._get_connection()
        try:
            cursor = await self._execute_with_retry(conn, """
                SELECT code_name FROM StockInfo WHERE code = ?
            """, (code,))
            row = await cursor.fetchone()
            return row[0] if row else ""
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åç§°å¤±è´¥: {str(e)}")
            raise

    async def save_stock_data(self, symbol: str, data: pd.DataFrame, frequency: str) -> bool:
        """ä¿å­˜è‚¡ç¥¨æ•°æ®åˆ°StockDataè¡¨ï¼ˆä¼˜åŒ–ç‰ˆ - è§£å†³é”å®šé—®é¢˜ï¼‰"""
        conn = None
        data_tmp = data.copy()
        data_tmp['date'] = pd.to_datetime(data_tmp['date'], format="%Y-%m-%d").dt.date

        # ç¡®ä¿ time åˆ—æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆå¤„ç† datetime.time å¯¹è±¡ï¼‰
        if 'time' in data_tmp.columns:
            # è®°å½•è½¬æ¢å‰çš„ç±»å‹ï¼Œç”¨äºè°ƒè¯•
            sample_time = data_tmp['time'].iloc[0] if len(data_tmp) > 0 else None
            logger.info(f"[save_stock_data] è½¬æ¢å‰ time åˆ—æ ·æœ¬: {sample_time}, ç±»å‹: {type(sample_time)}")

            data_tmp['time'] = data_tmp['time'].apply(
                lambda x: x.strftime('%H:%M:%S') if isinstance(x, datetime.time) else str(x) if pd.notna(x) else "00:00:00"
            )

            # è®°å½•è½¬æ¢åçš„ç±»å‹
            sample_time_after = data_tmp['time'].iloc[0] if len(data_tmp) > 0 else None
            logger.info(f"[save_stock_data] è½¬æ¢å time åˆ—æ ·æœ¬: {sample_time_after}, ç±»å‹: {type(sample_time_after)}")

        try:
            conn = await self._get_connection()
            records = data_tmp.to_dict('records')

            # å¤„ç†ä¸åŒé¢‘ç‡çš„æ•°æ®
            if frequency in ["1", "5", "15", "30", "60"]:
                # åˆ†é’Ÿçº§æ•°æ®æœ‰timeå­—æ®µï¼ˆå·²åœ¨å‰é¢è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
                insert_data = [
                    (
                        symbol,
                        record['date'].strftime('%Y-%m-%d') if hasattr(record['date'], 'strftime') else str(record['date']),
                        record.get('time', "00:00:00"),
                        record['open'],
                        record['high'],
                        record['low'],
                        record['close'],
                        record['volume'],
                        record.get('amount'),
                        record.get('adjustflag'),
                        frequency
                    )
                    for record in records
                ]
            else:
                # æ—¥çº¿åŠä»¥ä¸Šé¢‘ç‡æ•°æ®ï¼Œè®¾ç½®é»˜è®¤æ—¶é—´
                insert_data = [
                    (
                        symbol,
                        record['date'].strftime('%Y-%m-%d') if hasattr(record['date'], 'strftime') else str(record['date']),
                        "00:00:00",
                        record['open'],
                        record['high'],
                        record['low'],
                        record['close'],
                        record['volume'],
                        record.get('amount'),
                        record.get('adjustflag'),
                        frequency
                    )
                    for record in records
                ]

            # ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡ä»¥å‡å°‘é”å®šæ—¶é—´
            batch_size = min(100, self._batch_size)  # å¼ºåˆ¶æœ€å¤§æ‰¹æ¬¡ä¸º100
            total_batches = (len(insert_data) + batch_size - 1) // batch_size

            for i in range(0, len(insert_data), batch_size):
                batch = insert_data[i:i + batch_size]
                logger.debug(f"æ’å…¥ç¬¬{i//batch_size + 1}/{total_batches}æ‰¹æ•°æ®ï¼Œå…±{len(batch)}æ¡è®°å½•")

                await self._executemany_with_retry(conn, """
                    INSERT OR REPLACE INTO StockData (
                        code, date, time, open, high, low, close,
                        volume, amount, adjustflag, frequency
                    )
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, parameters=batch)

                # æ‰¹æ¬¡é—´æš‚åœï¼Œå‡å°‘é”å®šç«äº‰ï¼Œè®©å…¶ä»–æ“ä½œæœ‰æœºä¼šæ‰§è¡Œ
                if i + batch_size < len(insert_data):
                    await asyncio.sleep(0.1)  # å¢åŠ æš‚åœæ—¶é—´

            # éªŒè¯æ•°æ®æ˜¯å¦æˆåŠŸæ’å…¥
            logger.info(f"æˆåŠŸä¿å­˜{symbol}çš„{frequency}é¢‘ç‡æ•°æ®ï¼Œå…±{len(insert_data)}æ¡è®°å½•")

            # æ˜¾å¼æäº¤äº‹åŠ¡
            await conn.commit()
            logger.debug(f"äº‹åŠ¡å·²æäº¤ï¼Œä¿å­˜{symbol}çš„{frequency}é¢‘ç‡æ•°æ®")

            # ç®€å•éªŒè¯ï¼šæŸ¥è¯¢åˆšæ’å…¥çš„è®°å½•æ•°
            verify_cursor = await self._execute_with_retry(conn,
                "SELECT COUNT(*) FROM StockData WHERE code = ? AND frequency = ?",
                (symbol, frequency)
            )
            count = await verify_cursor.fetchone()
            logger.info(f"æ•°æ®åº“ä¸­{symbol}çš„{frequency}é¢‘ç‡è®°å½•æ€»æ•°: {count[0] if count else 0}")

            return True

        except Exception as e:
            logger.error(f"ä¿å­˜è‚¡ç¥¨æ•°æ®å¤±è´¥: {str(e)}")
            if conn:
                try:
                    await conn.rollback()
                    logger.debug("äº‹åŠ¡å·²å›æ»š")
                except Exception as rollback_error:
                    logger.error(f"äº‹åŠ¡å›æ»šå¤±è´¥: {rollback_error}")
            raise
        finally:
            # è¿æ¥æ± ä¸­çš„è¿æ¥ä¸éœ€è¦æ˜¾å¼å…³é—­ï¼Œè¿æ¥æ± ä¼šè‡ªåŠ¨ç®¡ç†
            pass

    async def save_money_supply_data(self, data: pd.DataFrame) -> bool:
        """ä¿å­˜è´§å¸ä¾›åº”é‡æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        conn = await self._get_connection()
        try:
            records = data.to_dict('records')
            insert_data = [
                (
                    record['statMonth'],
                    record['m2'],
                    record['m2YoY'],
                    record['m1'],
                    record['m1YoY'],
                    record['m0'],
                    record['m0YoY'],
                    record['cd'],
                    record['cdYoY'],
                    record['qm'],
                    record['qmYoY'],
                    record['ftd'],
                    record['ftdYoY'],
                    record['sd'],
                    record['sdYoY']
                )
                for record in records
            ]

            await self._executemany_with_retry(conn, """
                INSERT OR REPLACE INTO MoneySupplyData (
                    stat_month, m2, m2_yoy, m1, m1_yoy, m0, m0_yoy,
                    cd, cd_yoy, qm, qm_yoy, ftd, ftd_yoy, sd, sd_yoy
                )
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, parameters=insert_data)

            logger.info(f"æˆåŠŸä¿å­˜{len(insert_data)}æ¡è´§å¸ä¾›åº”é‡æ•°æ®")
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜è´§å¸ä¾›åº”é‡æ•°æ®å¤±è´¥: {str(e)}")
            raise

    async def get_money_supply_data(self, start_month: str, end_month: str) -> pd.DataFrame:
        """è·å–è´§å¸ä¾›åº”é‡æ•°æ®"""
        conn = await self._get_connection()
        try:
            cursor = await self._execute_with_retry(conn, """
                SELECT * FROM MoneySupplyData
                WHERE stat_month BETWEEN ? AND ?
                ORDER BY stat_month
            """, (start_month, end_month))

            rows = await cursor.fetchall()

            if not rows:
                logger.warning(f"æœªæ‰¾åˆ°{start_month}è‡³{end_month}çš„è´§å¸ä¾›åº”é‡æ•°æ®")
                return pd.DataFrame()

            df = pd.DataFrame(rows, columns=[
                'id', 'stat_month', 'm2', 'm2_yoy', 'm1', 'm1_yoy',
                'm0', 'm0_yoy', 'cd', 'cd_yoy', 'qm', 'qm_yoy',
                'ftd', 'ftd_yoy', 'sd', 'sd_yoy'
            ])

            # ç§»é™¤idåˆ—ï¼Œä¸PostgreSQLç‰ˆæœ¬ä¿æŒä¸€è‡´
            if 'id' in df.columns:
                df = df.drop('id', axis=1)

            logger.info(f"æˆåŠŸè·å–{len(df)}æ¡è´§å¸ä¾›åº”é‡æ•°æ®")
            return df

        except Exception as e:
            logger.error(f"è·å–è´§å¸ä¾›åº”é‡æ•°æ®å¤±è´¥: {str(e)}")
            raise

    def get_pool_status(self) -> dict:
        """è·å–è¿æ¥æ± çŠ¶æ€"""
        return {
            "db_type": "sqlite",
            "db_path": self.db_path,
            "initialized": self._initialized,
            "pool_size": len(self.pools),
            "max_connections": self._max_connections,
            "current_connection_index": self._pool_index,
            "busy_timeout": self._busy_timeout,
            "connected": len(self.pools) > 0
        }